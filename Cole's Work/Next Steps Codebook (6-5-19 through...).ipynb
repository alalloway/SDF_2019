{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Download and Standardize Script**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this first\n",
    "### imports neccessary modules and starting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import string\n",
    "import arcpy\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import csv\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "from zipfile import BadZipfile\n",
    "from glob import glob\n",
    "\n",
    "theURL = \"https://www2.census.gov/geo/tiger/TIGER2018\"\n",
    "#starting, anyway\n",
    "working_directory = \"C:\\Users\\and04671\\Documents\\Data_Files\"\n",
    "out_db = \"C:\\Users\\and04671\\Documents\\Data_Files\\TIGER2018.gdb\"\n",
    "\n",
    "webfolders = [\n",
    "#standard for 2017    \n",
    "           'AIANNH/','AITSN/','ANRC/','BG/','CONCITY/','COUSUB/','CNECTA/',\n",
    "           'CBSA/','CD/','COUNTY/','CSA/','CSA/','METDIV/','NECTA/',\n",
    "           'NECTADIV/','PLACE/','PUMA/','ELSD/','SCSD/','UNSD/',\n",
    "           'STATE/','SLDL/','SLDU/','SUBMCD/','TBG/','TRACT/','TTRACT/',\n",
    "           'UAC/','ZCTA5/',\n",
    "#new for 2018\n",
    "           'AREAWATER/','LINEARWATER/','COASTLINE/','FACES/',\n",
    "           'FACESAH/','FACESAL/','FACESMIL/','FEATNAMES/','PRIMARYROADS/',\n",
    "           'PRISECROADS/','RAILS/','ROADS/']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io, csv, os\n",
    "from bs4 import BeautifulSoup\n",
    "from zipfile import BadZipfile\n",
    "\n",
    "# Where you want the files to download to (will create new folders within this workspace) \n",
    "working_directory = \"C:\\Users\\and04671\\Documents\\Data_Files\"\n",
    "\n",
    "# The census website. NOTE: this URL will have to be updated to '.../TIGER2019/' and so on, each year\n",
    "theUrl = \"https://www2.census.gov/geo/tiger/TIGER2018/\"\n",
    "\n",
    "# Census download site folder names below\n",
    "#NOTE: if the folder names change on the census download directory for a new year, change the names in this list\n",
    "\n",
    "folders = ['AIANNH/','AITSN/','ANRC/','BG/','CONCITY/','COUSUB','CNECTA/','CBSA/','CD/',\n",
    "           'COUNTY/','CSA/','CSA/','METDIV/','NECTA/','NECTADIV/',\n",
    "           'PLACE/','PUMA/','ELSD/','SCSD/','UNSD/','STATE/','SLDL/','SLDU/',\n",
    "           'SUBMCD/','TBG/','TRACT/','TTRACT/','UAC/','ZCTA5/',\n",
    "            \n",
    "           #these don't seem to be in 2017 list but want to keep \n",
    "           'AREAWATER/','LINEARWATER/','COASTLINE/','FACES/',\n",
    "           'FACESAH/','FACESAL/','FACESMIL/','FEATNAMES/','PRIMARYROADS/',\n",
    "           'PRISECROADS/','RAILS/','ROADS/']\n",
    "\n",
    "for webfolder in webfolders:\n",
    "    fullUrl = theUrl + webfolder\n",
    "    r = requests.get(fullUrl)\n",
    "    webpage = BeautifulSoup(r.text, \"html.parser\")\n",
    "    table = webpage.find(\"table\")\n",
    "    # Find all table row (tr rows)\n",
    "    tr = table.find_all(\"tr\")\n",
    "    #print(tr)\n",
    "    hrefs = []\n",
    "    # Skipping headers\n",
    "    for each_tr in tr[3:]:\n",
    "        td = each_tr.find_all('td')\n",
    "        # In each tr row find each td cell\n",
    "        for each_td in td:\n",
    "            #print(each_td.text) # If you want to check that you are downloading the correct census shapefiles\n",
    "            if each_td.find('a'): hrefs.append(each_td.find('a')['href'])\n",
    "    print(\"Downloading and Extracting files\")\n",
    "    \n",
    "    \n",
    "    for c, h in enumerate(hrefs):\n",
    "        print(\"Downloading %s of %s\" % (c+1, len(hrefs)) )\n",
    "        urlZip = \"%s/%s\" % (fullUrl, h)\n",
    "        theZip = requests.get(urlZip)\n",
    "        try:\n",
    "            z = zipfile.ZipFile(io.BytesIO(theZip.content))\n",
    "            # Creates a file folder to put each type of data into, inside of theShapefileDirectory specified above\n",
    "            if not os.path.exists(os.path.join(theShapeFileDirectory,webfolder[:-1])):\n",
    "                os.makedirs(os.path.join(theShapeFileDirectory,webfolder[:-1]))\n",
    "                destination = (os.path.join(theShapeFileDirectory,webfolder[:-1]))\n",
    "            # if the file folder with that name already exists, it won't duplicate\n",
    "            else:\n",
    "                destination = (os.path.join(theShapeFileDirectory,webfolder[:-1]))\n",
    "            z.extractall(destination)\n",
    "        except BadZipfile:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy shapes to geodatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Not signed into Portal.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d97d0a2e7f0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0marcpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#constants\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0m_initagsenv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0marcpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeoprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0marcpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeoprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0marcpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeoprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgptooldoc\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_gptooldoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\geoprocessing\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#email: contracts@esri.com\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\geoprocessing\\_base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#email: contracts@esri.com\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0marcgisscripting\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Not signed into Portal."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import string\n",
    "import arcpy\n",
    "\n",
    "#constants\n",
    "working_directory = r\"C:\\Users\\and04671\\Documents\\Data_Files\"\n",
    "#this dirs list includes all directories defined in the download script, without \n",
    "#additional water/road directories\n",
    "folders = ['AIANNH','AITSN','ANRC','BG','CBSA','CD','CNECTA','CONCITY','COUNTY',\n",
    "        'COUSUB','CSA','ELSD','METDIV','NECTA','NECTADIV','PLACE','PUMA',\n",
    "        'SCSD','SLDL','SLDU','STATE','SUBMCD','TBG','TRACT','TTRACT','UAC',\n",
    "        'UNSD','ZCTA5',\n",
    "          \n",
    "        'AREAWATER','LINEARWATER','COASTLINE','FACES',\n",
    "        'FACESAH','FACESAL','FACESMIL','FEATNAMES','PRIMARYROADS',\n",
    "        'PRISECROADS','RAILS','ROADS']\n",
    "#dirs = ['AIANNH']\n",
    "#dirs = ['COUSB']\n",
    "out_db = r\"C:3rs\\and04671\\Documents\\Data_Files\\TIGER2018.gdb\"\n",
    "\n",
    "#0. get the shapefiles into the geodb, merging shapefiles that come as state-wide extents\n",
    "for folder in folders:\n",
    "    shp_dir = working_directory + folder + \"/\"\n",
    "    \n",
    "    # list all shapefiles in directory\n",
    "    arcpy.env.workspace = shp_dir\n",
    "    shps = arcpy.ListFeatureClasses(\"*\")\n",
    "    if len(shps) == 1:\n",
    "        shp_file = shp_dir + shps[0]\n",
    "        shp_name = shps[0]\n",
    "        shp_name = shp_name.strip(\"shp\")\n",
    "        out_fc = out_db + shp_name\n",
    "        arcpy.CopyFeatures_management(shp_file,out_fc)\n",
    "    else:\n",
    "        out_fc = out_db + \"tl_2018_us_\" + folder.lower()\n",
    "        arcpy.Merge_management(shps,out_fc)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add GISjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Not signed into Portal.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b6ff03932985>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0marcpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#see GIS JOIN pattern for # reasoning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mworking_directory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"C:\\Users\\and04671\\Documents\\Data_Files\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0m_initagsenv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0marcpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeoprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0marcpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeoprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0marcpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeoprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgptooldoc\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_gptooldoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\geoprocessing\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#email: contracts@esri.com\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\geoprocessing\\_base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#email: contracts@esri.com\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0marcgisscripting\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Not signed into Portal."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import string\n",
    "import arcpy\n",
    "#see GIS JOIN pattern for # reasoning\n",
    "working_directory = r\"C:\\Users\\and04671\\Documents\\Data_Files\"\n",
    "\n",
    "out_db = r\"C:\\Users\\and04671\\Documents\\Data_Files\\TIGER2018.gdb\"\n",
    "\n",
    "dir_dict = {'AIANHH':5,'AITSN':8,'ANRC':6,'BG':15,'CBSA':6,'CD115':6,'CNECTA':4,'CONCITY':9,'COUNTY':8,\n",
    "            'COUSUB':13,'CSA':4,'DIVISION':2,'ELSD':9,'METDIV':11,'NATION':2,'NECTA':6,'NECTADIV':11,\n",
    "            'PLACE':9,'PUMA':9,'SCSD':9,'SLDL':7,'SLDU':7,'STATE':4,'SUBMCD':18,'TBG':12,'TRACT':14,\n",
    "            'TTRACT':11,'UAC10':6,'UNSD':9,'ZCTA510':6,'TRUST':5,'RES_ONLY':5,'REGION':2}\n",
    "\n",
    "'''     'AREAWATER','LINEARWATER','COASTLINE','FACES',\n",
    "        'FACESAH','FACESAL','FACESMIL','FEATNAMES','PRIMARYROADS',\n",
    "        'PRISECROADS','RAILS','ROADS'\n",
    "'''\n",
    "\n",
    "new_field = \"GISJOIN\"\n",
    "arcpy.env.workspace = out_db\n",
    "\n",
    "for key, textfield in dir_dict.items():\n",
    "    #this looks at the (key,item) pairs in the dict\n",
    "    key_lower = key.lower()\n",
    "    fc = arcpy.ListFeatureClasses(\"*\"+key_lower)\n",
    "    fc_path = out_db + fc[0]\n",
    "    arcpy.AddField_management(fc_path,new_field,\"TEXT\",\"\",\"\",textfield,\"\",\"\",\"\",\"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprojection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import arcpy\n",
    "\n",
    "# Source location - where are the shapefiles coming from? \n",
    "# Change this source location to your source location\n",
    "# Make sure to copy file paths in using \\\\ or / as path separators instead of \\ (escape key) or use r (raw) to escape \n",
    "src = \"C:\\\\Users\\\\allow001\\\\work\\\\scratchdata\\\\hydro_shp\" \n",
    "\n",
    "#glob looks for files ending in .shp in the specificed folder location from src (source)\n",
    "for file in glob(os.path.join(src, \"*.shp\")):\n",
    "    #print(file[-19:-17]+'_water_poly.shp')\n",
    "    #for file in glob(os.path.join(folder,\"???_arc.shp\")):\n",
    "        #print(file)\n",
    "    # input data is in GCS, not projected\n",
    "    input_features = file\n",
    "        # output data\n",
    "    # rename each output - I typically just add \"proj\" or \"projected\" to the original name \n",
    "    output_feature_class = os.path.join(src,file[-19:-17]+'_water_poly.shp')\n",
    "    print(output_feature_class)\n",
    "    # set the Coordinate System - don't change \n",
    "    out_coordinate_system = arcpy.SpatialReference('USA Contiguous Albers Equal Area Conic')\n",
    "    # run the tool\n",
    "    arcpy.Project_management(input_features, output_feature_class, out_coordinate_system)\n",
    "\n",
    "\n",
    "# create a spatial reference object for the output coordinate system\n",
    "#out_coordinate_system = arcpy.SpatialReference('USA Contiguous Albers Equal Area Conic')\n",
    "\n",
    "# run the tool\n",
    "#arcpy.Project_management(input_features, output_feature_class, out_coordinate_system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Water merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import string\n",
    "import arcpy\n",
    "\n",
    "#constants\n",
    "working_directory = \"C:\\Users\\and04671\\Documents\\Data_Files\"\n",
    "out_db = \"C:\\Users\\and04671\\Documents\\Data_Files\\TIGER2018_water.gdb\"\n",
    "water = 'AREAWATER'\n",
    "\n",
    "shp_dir = wd + water + \"/\"\n",
    "#\n",
    "\n",
    "arcpy.env.workspace = shp_dir\n",
    "shps = arcpy.ListFeatureClasses(\"*\")\n",
    "out_fc = out_db + \"tl_2017_us_\" + water.lower()\n",
    "arcpy.Merge_management(shps,out_fc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename Statewide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This script renames the state-specific block group feature classes by\n",
    "# postal abbreviations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import string\n",
    "import arcpy\n",
    "\n",
    "working_directory = r\"C:\\Users\\and04671\\Documents\\Data_Files\"\n",
    "geodb = working_directory + \"TIGER2018_proj_watererase.gdb\"\n",
    "out_fc_stub = \"_blck_grp_2018\"\n",
    "\n",
    "state_codes = {\n",
    " '53':'WA',\n",
    " '10':'DE',\n",
    " '11':'DC',\n",
    " '55':'WI',\n",
    " '54':'WV',\n",
    " '15':'HI',\n",
    " '12':'FL',\n",
    " '56':'WY',\n",
    " '72':'PR',\n",
    " '34':'NJ',\n",
    " '35':'NM',\n",
    " '48':'TX',\n",
    " '22':'LA',\n",
    " '37':'NC',\n",
    " '38':'ND',\n",
    " '31':'NE',\n",
    " '47':'TN',\n",
    " '36':'NY',\n",
    " '42':'PA',\n",
    " '02':'AK',\n",
    " '32':'NV',\n",
    " '33':'NH',\n",
    " '51':'VA',\n",
    " '08':'CO',\n",
    " '06':'CA',\n",
    " '01':'AL',\n",
    " '05':'AR',\n",
    " '50':'VT',\n",
    " '17':'IL',\n",
    " '13':'GA',\n",
    " '18':'IN',\n",
    " '19':'IA',\n",
    " '25':'MA',\n",
    " '04':'AZ',\n",
    " '16':'ID',\n",
    " '09':'CT',\n",
    " '23':'ME',\n",
    " '24':'MD',\n",
    " '40':'OK',\n",
    " '39':'OH',\n",
    " '49':'UT',\n",
    " '29':'MO',\n",
    " '27':'MN',\n",
    " '26':'MI',\n",
    " '44':'RI',\n",
    " '20':'KS',\n",
    " '30':'MT',\n",
    " '28':'MS',\n",
    " '45':'SC',\n",
    " '21':'KY',\n",
    " '41':'OR',\n",
    " '46':'SD',\n",
    " '72':'PR' \n",
    "}\n",
    "\n",
    "arcpy.env.workspace = geodb\n",
    "fcs = arcpy.ListFeatureClasses(\"bg_*\")\n",
    "\n",
    "for fc in fcs:\n",
    "    fc_name = fc.split(\"_\")\n",
    "    fc_state = fc_name[1]\n",
    "    state_abbr = state_codes.get(fc_state)\n",
    "    out_fc = state_abbr + out_fc_stub\n",
    "    arcpy.Rename_management(fc,out_fc)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to SHP .zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This script exports all fcs in a geodb to shapefiles and ZIPS them using the\n",
    "# NHGIS naming convention\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import zipfile\n",
    "import string\n",
    "import arcpy\n",
    "\n",
    "working_directory = r\"C:\\Users\\and04671\\Documents\\Data_Files\"\n",
    "#geodb = wd + \"TIGER2018_proj_watererase.gdb\"\n",
    "out_folder = working_directory + \"shps/\"\n",
    "\n",
    "#arcpy.env.workspace = geodb\n",
    "#fcs = arcpy.ListFeatureClasses(\"*\")\n",
    "\n",
    "#for fc in fcs:\n",
    "#    out_shp = fc + \".shp\"\n",
    "#    arcpy.FeatureClassToFeatureClass_conversion(fc,out_folder,out_shp)\n",
    "\n",
    "os.chdir(out_folder)\n",
    "\n",
    "shps = glob.glob(out_folder + \"*.shp\")\n",
    "\n",
    "ziplist = []\n",
    "\n",
    "for name in shps:\n",
    "    file = os.path.basename(name)\n",
    "    names = file[:-4]\n",
    "    ziplist.append(names)\n",
    "\n",
    "for f in ziplist:\n",
    "    file_name = os.path.join(out_folder,f + \".zip\")\n",
    "    files = glob.glob(str(f)+\".*\")\n",
    "    zips = zipfile.ZipFile(file_name,'w')\n",
    "    for s in files:\n",
    "        zips.write(s, compress_type = zipfile.ZIP_DEFLATED)\n",
    "    zips.close()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename .zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This script renames shapefiles to lower case 'us' and three character\n",
    "# fips codes. \n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import zipfile\n",
    "import string\n",
    "#import arcpy\n",
    "\n",
    "states_upper = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
    "          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
    "          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
    "          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\", \"PR\"]\n",
    "\n",
    "state_codes = {\n",
    "    'WA': '530',\n",
    "    'DE': '100',\n",
    "    'DC': '110',\n",
    "    'WI': '550',\n",
    "    'WV': '540',\n",
    "    'HI': '150',\n",
    "    'FL': '120',\n",
    "    'WY': '560',\n",
    "    'PR': '720',\n",
    "    'NJ': '340',\n",
    "    'NM': '350',\n",
    "    'TX': '480',\n",
    "    'LA': '220',\n",
    "    'NC': '370',\n",
    "    'ND': '380',\n",
    "    'NE': '310',\n",
    "    'TN': '470',\n",
    "    'NY': '360',\n",
    "    'PA': '420',\n",
    "    'AK': '020',\n",
    "    'NV': '320',\n",
    "    'NH': '330',\n",
    "    'VA': '510',\n",
    "    'CO': '080',\n",
    "    'CA': '060',\n",
    "    'AL': '010',\n",
    "    'AR': '050',\n",
    "    'VT': '500',\n",
    "    'IL': '170',\n",
    "    'GA': '130',\n",
    "    'IN': '180',\n",
    "    'IA': '190',\n",
    "    'MA': '250',\n",
    "    'AZ': '040',\n",
    "    'ID': '160',\n",
    "    'CT': '090',\n",
    "    'ME': '230',\n",
    "    'MD': '240',\n",
    "    'OK': '400',\n",
    "    'OH': '390',\n",
    "    'UT': '490',\n",
    "    'MO': '290',\n",
    "    'MN': '270',\n",
    "    'MI': '260',\n",
    "    'RI': '440',\n",
    "    'KS': '200',\n",
    "    'MT': '300',\n",
    "    'MS': '280',\n",
    "    'SC': '450',\n",
    "    'KY': '210',\n",
    "    'OR': '410',\n",
    "    'SD': '460',\n",
    "    'PR': '720'\n",
    "}\n",
    "\n",
    "SHPwd = \"C:\\Users\\and04671\\Documents\\Data_Files\\shps\"\n",
    "\n",
    "os.chdir(wd)\n",
    "\n",
    "zips = glob.glob(wd + \"US*.zip\")\n",
    "\n",
    "ziplist = []\n",
    "\n",
    "for name in zips:\n",
    "    file = os.path.basename(name)\n",
    "    ziplist.append(file)\n",
    "\n",
    "for f in ziplist:\n",
    "    new_zip = f.lower()\n",
    "    os.rename(f,new_zip)\n",
    "\n",
    "for state in states_upper:\n",
    "    in_zip = glob.glob(wd + state + \"*.zip\")\n",
    "    in_zip_string = in_zip[0]\n",
    "    in_zip_file = os.path.basename(in_zip_string)\n",
    "\n",
    "    out_zip_file = state_codes.get(state) + \"_blck_grp_2017.zip\"\n",
    "    os.rename(in_zip_file, out_zip_file)\n",
    "    \n",
    "                 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
