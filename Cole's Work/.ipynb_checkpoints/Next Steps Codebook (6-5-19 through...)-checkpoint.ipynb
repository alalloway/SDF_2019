{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io, csv, os\n",
    "from bs4 import BeautifulSoup\n",
    "from zipfile import BadZipfile\n",
    "\n",
    "# Where you want the files to download to (will create new folders within this workspace) \n",
    "theShapeFileDirectory = r\"C:\\Users\\and04671\\Documents\\Data_Files\"\n",
    "\n",
    "# The census website. NOTE: this URL will have to be updated to '.../TIGER2019/' and so on, each year\n",
    "theUrl = \"https://www2.census.gov/geo/tiger/TIGER2018/\"\n",
    "\n",
    "# Census download site folder names below\n",
    "#NOTE: if the folder names change on the census download directory for a new year, change the names in this list\n",
    "\n",
    "folders = ['AIANNH/','AITSN/','ANRC/','BG/','CONCITY/','COUSUB','CNECTA/','CBSA/','CD/',\n",
    "           'COUNTY/','CSA/','CSA/','METDIV/','NECTA/','NECTADIV/',\n",
    "           'PLACE/','PUMA/','ELSD/','SCSD/','UNSD/','STATE/','SLDL/','SLDU/',\n",
    "           'SUBMCD/','TBG/','TRACT/','TTRACT/','UAC/','ZCTA5/',\n",
    "            \n",
    "           #these don't seem to be in 2017 list but want to keep \n",
    "           'AREAWATER/','LINEARWATER/','COASTLINE/','FACES/',\n",
    "           'FACESAH/','FACESAL/','FACESMIL/','FEATNAMES/','PRIMARYROADS/',\n",
    "           'PRISECROADS/','RAILS/','ROADS/']\n",
    "\n",
    "for folder in folders:\n",
    "    fullUrl = theUrl + folder\n",
    "    r = requests.get(fullUrl)\n",
    "    webpage = BeautifulSoup(r.text, \"html.parser\")\n",
    "    table = webpage.find(\"table\")\n",
    "    # Find all table row (tr rows)\n",
    "    tr = table.find_all(\"tr\")\n",
    "    #print(tr)\n",
    "    hrefs = []\n",
    "    # Skipping headers\n",
    "    for each_tr in tr[3:]:\n",
    "        td = each_tr.find_all('td')\n",
    "        # In each tr row find each td cell\n",
    "        for each_td in td:\n",
    "            #print(each_td.text) # If you want to check that you are downloading the correct census shapefiles\n",
    "            if each_td.find('a'): hrefs.append(each_td.find('a')['href'])\n",
    "    print(\"Downloading and Extracting files\")\n",
    "    \n",
    "    \n",
    "    for c, h in enumerate(hrefs):\n",
    "        print(\"Downloading %s of %s\" % (c+1, len(hrefs)) )\n",
    "        urlZip = \"%s/%s\" % (fullUrl, h)\n",
    "        theZip = requests.get(urlZip)\n",
    "        try:\n",
    "            z = zipfile.ZipFile(io.BytesIO(theZip.content))\n",
    "            # Creates a file folder to put each type of data into, inside of theShapefileDirectory specified above\n",
    "            if not os.path.exists(os.path.join(theShapeFileDirectory,folder[:-1])):\n",
    "                os.makedirs(os.path.join(theShapeFileDirectory,folder[:-1]))\n",
    "                destination = (os.path.join(theShapeFileDirectory,folder[:-1]))\n",
    "            # if the file folder with that name already exists, it won't duplicate\n",
    "            else:\n",
    "                destination = (os.path.join(theShapeFileDirectory,folder[:-1]))\n",
    "            z.extractall(destination)\n",
    "        except BadZipfile:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy shapes to geodatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import string\n",
    "import arcpy\n",
    "\n",
    "#constants\n",
    "working_directory = \"C:\\Users\\and04671\\Documents\\Data_Files\"\n",
    "#this dirs list includes all directories defined in the download script, without \n",
    "#additional water/road directories\n",
    "folders = ['AIANNH','AITSN','ANRC','BG','CBSA','CD','CNECTA','CONCITY','COUNTY',\n",
    "        'COUSUB','CSA','ELSD','METDIV','NECTA','NECTADIV','PLACE','PUMA',\n",
    "        'SCSD','SLDL','SLDU','STATE','SUBMCD','TBG','TRACT','TTRACT','UAC',\n",
    "        'UNSD','ZCTA5']\n",
    "#dirs = ['AIANNH']\n",
    "#dirs = ['COUSUB']\n",
    "out_db = r\"C:\\Users\\and04671\\Documents\\Data_Files\\TIGER2018.gdb\"\n",
    "\n",
    "#0. get the shapefiles into the geodb, merging shapefiles that come as state-wide extents\n",
    "for folder in folders:\n",
    "    shp_dir = working_directory + folder + \"\\\"\n",
    "    \n",
    "    # list all shapefiles in directory\n",
    "    arcpy.env.workspace = shp_dir\n",
    "    shps = arcpy.ListFeatureClasses(\"*\")\n",
    "    if len(shps) == 1:\n",
    "        shp_file = shp_dir + shps[0]\n",
    "        shp_name = shps[0]\n",
    "        shp_name = shp_name.strip(\"shp\")\n",
    "        out_fc = out_db + shp_name\n",
    "        arcpy.CopyFeatures_management(shp_file,out_fc)\n",
    "    else:\n",
    "        out_fc = out_db + \"tl_2017_us_\" + dir.lower()\n",
    "        arcpy.Merge_management(shps,out_fc)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add GISjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import string\n",
    "import arcpy\n",
    "#G-\"State Code (2)\"-0-\"County Code(3)\"-0\n",
    "wd = \"D:/workspace/TIGER2017/\"\n",
    "out_db = \"D:/workspace/TIGER2017/TIGER2017_proj.gdb/\"\n",
    "dir_dict = {'AIANHH':5,'AITSN':8,'ANRC':6,'BG':15,'CBSA':6,'CD115':6,'CNECTA':4,'CONCITY':9,'COUNTY':8,'COUSUB':13,'CSA':4,'DIVISION':2,'ELSD':9,'METDIV':11,'NATION':2,'NECTA':6,'NECTADIV':11,'PLACE':9,'PUMA':9,'SCSD':9,'SLDL':7,'SLDU':7,'STATE':4,'SUBMCD':18,'TBG':12,'TRACT':14,'TTRACT':11,'UAC10':6,'UNSD':9,'ZCTA510':6,'TRUST':5,'RES_ONLY':5,'REGION':2}\n",
    "#dir_dict = {'PLACE':9,'PUMA':9,'SCSD':9,'SLDL':7,'SLDU':7,'STATE':4,'SUBMCD':18,'TBG':12,'UAC':6,'UNSD':9,'ZCTA5':6,'TRUST':5,'RES_ONLY':5,'REGION':2}\n",
    "#dir_dict = {'CD115': 6, 'COUNTY': 8, 'COUSUB': 13, 'STATE': 4, 'CBSA': 6, 'PLACE': 9, 'ELSD': 9, 'ANRC': 6, 'SLDL': 7, 'NATION': 2, 'NECTADIV': 11, 'AIANHH': 5, 'CNECTA': 4, 'PUMA': 9, 'TTRACT': 11, 'REGION': 2, 'AITSN': 8, 'METDIV': 11, 'UNSD': 9, 'TBG': 12, 'SCSD': 9, 'ZCTA5': 6, 'UAC': 6, 'TRUST': 5, 'NECTA': 6}\n",
    "#dir_dict = {'TTRACT': 11, 'TRUST': 5, 'STATE': 4, 'REGION': 2, 'PUMA': 9, 'PLACE': 9, 'NECTA': 6\n",
    "new_field = \"GISJOIN\"\n",
    "arcpy.env.workspace = out_db\n",
    "\n",
    "for j,i in dir_dict.items():\n",
    "    j_lower = j.lower()\n",
    "    fc = arcpy.ListFeatureClasses(\"*\"+j_lower)\n",
    "    fc_path = out_db + fc[0]\n",
    "    arcpy.AddField_management(fc_path,new_field,\"TEXT\",\"\",\"\",i,\"\",\"\",\"\",\"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import string\n",
    "import arcpy\n",
    "\n",
    "#constants\n",
    "wd = \"D:/workspace/TIGER2017/\"\n",
    "out_db = \"D:/workspace/TIGER2017/TIGER2017_water.gdb/\"\n",
    "dir = 'AREAWATER'\n",
    "\n",
    "shp_dir = wd + dir + \"/\"\n",
    "arcpy.env.workspace = shp_dir\n",
    "shps = arcpy.ListFeatureClasses(\"*\")\n",
    "out_fc = out_db + \"tl_2017_us_\" + dir.lower()\n",
    "arcpy.Merge_management(shps,out_fc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename Statewide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "# 4_rename_statewide_bg_fc.py\n",
    "# \n",
    "# David Van Riper\n",
    "#\n",
    "# 12/14/2017\n",
    "# \n",
    "# This script renames the state-specific block group feature classes by\n",
    "# postal abbreviations\n",
    "#\n",
    "# Local processing works great!!!\n",
    "##########################################################################################\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import string\n",
    "import arcpy\n",
    "\n",
    "wd = \"D:/workspace/TIGER2017/\"\n",
    "geodb = wd + \"TIGER2017_proj_watererase.gdb\"\n",
    "out_fc_stub = \"_blck_grp_2017\"\n",
    "\n",
    "state_codes = {\n",
    " '53':'WA',\n",
    " '10':'DE',\n",
    " '11':'DC',\n",
    " '55':'WI',\n",
    " '54':'WV',\n",
    " '15':'HI',\n",
    " '12':'FL',\n",
    " '56':'WY',\n",
    " '72':'PR',\n",
    " '34':'NJ',\n",
    " '35':'NM',\n",
    " '48':'TX',\n",
    " '22':'LA',\n",
    " '37':'NC',\n",
    " '38':'ND',\n",
    " '31':'NE',\n",
    " '47':'TN',\n",
    " '36':'NY',\n",
    " '42':'PA',\n",
    " '02':'AK',\n",
    " '32':'NV',\n",
    " '33':'NH',\n",
    " '51':'VA',\n",
    " '08':'CO',\n",
    " '06':'CA',\n",
    " '01':'AL',\n",
    " '05':'AR',\n",
    " '50':'VT',\n",
    " '17':'IL',\n",
    " '13':'GA',\n",
    " '18':'IN',\n",
    " '19':'IA',\n",
    " '25':'MA',\n",
    " '04':'AZ',\n",
    " '16':'ID',\n",
    " '09':'CT',\n",
    " '23':'ME',\n",
    " '24':'MD',\n",
    " '40':'OK',\n",
    " '39':'OH',\n",
    " '49':'UT',\n",
    " '29':'MO',\n",
    " '27':'MN',\n",
    " '26':'MI',\n",
    " '44':'RI',\n",
    " '20':'KS',\n",
    " '30':'MT',\n",
    " '28':'MS',\n",
    " '45':'SC',\n",
    " '21':'KY',\n",
    " '41':'OR',\n",
    " '46':'SD',\n",
    " '72':'PR' \n",
    "}\n",
    "\n",
    "arcpy.env.workspace = geodb\n",
    "fcs = arcpy.ListFeatureClasses(\"bg_*\")\n",
    "\n",
    "for fc in fcs:\n",
    "    fc_name = fc.split(\"_\")\n",
    "    fc_state = fc_name[1]\n",
    "    state_abbr = state_codes.get(fc_state)\n",
    "    out_fc = state_abbr + out_fc_stub\n",
    "    arcpy.Rename_management(fc,out_fc)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to SHP zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "# 5_export_to_shp_and_zip.py\n",
    "# \n",
    "# David Van Riper\n",
    "#\n",
    "# 12/15/2017\n",
    "#\n",
    "# This script exports all fcs in a geodb to shapefiles and ZIPS them using the\n",
    "# NHGIS naming convention\n",
    "#\n",
    "# Local processing works great!!!\n",
    "##########################################################################################\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import zipfile\n",
    "import string\n",
    "import arcpy\n",
    "\n",
    "wd = \"D:/workspace/TIGER2017/\"\n",
    "#geodb = wd + \"TIGER2016_proj_watererase.gdb\"\n",
    "out_folder = wd + \"shps/\"\n",
    "\n",
    "#arcpy.env.workspace = geodb\n",
    "#fcs = arcpy.ListFeatureClasses(\"*\")\n",
    "\n",
    "#for fc in fcs:\n",
    "#    out_shp = fc + \".shp\"\n",
    "#    arcpy.FeatureClassToFeatureClass_conversion(fc,out_folder,out_shp)\n",
    "\n",
    "os.chdir(out_folder)\n",
    "\n",
    "shps = glob.glob(out_folder + \"*.shp\")\n",
    "\n",
    "ziplist = []\n",
    "\n",
    "for name in shps:\n",
    "    file = os.path.basename(name)\n",
    "    names = file[:-4]\n",
    "    ziplist.append(names)\n",
    "\n",
    "for f in ziplist:\n",
    "    file_name = os.path.join(out_folder,f + \".zip\")\n",
    "    files = glob.glob(str(f)+\".*\")\n",
    "    zips = zipfile.ZipFile(file_name,'w')\n",
    "    for s in files:\n",
    "        zips.write(s, compress_type = zipfile.ZIP_DEFLATED)\n",
    "    zips.close()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename ZIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "# 6_rename_zipfiles.py\n",
    "# \n",
    "# David Van Riper\n",
    "#\n",
    "# 12/15/2017\n",
    "#\n",
    "# This script renames shapefiles to lower case 'us' and three character\n",
    "# fips codes. \n",
    "#\n",
    "# Local processing works great!!!\n",
    "##########################################################################################\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import zipfile\n",
    "import string\n",
    "#import arcpy\n",
    "\n",
    "states_upper = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
    "          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
    "          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
    "          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\", \"PR\"]\n",
    "\n",
    "state_codes = {\n",
    "    'WA': '530',\n",
    "    'DE': '100',\n",
    "    'DC': '110',\n",
    "    'WI': '550',\n",
    "    'WV': '540',\n",
    "    'HI': '150',\n",
    "    'FL': '120',\n",
    "    'WY': '560',\n",
    "    'PR': '720',\n",
    "    'NJ': '340',\n",
    "    'NM': '350',\n",
    "    'TX': '480',\n",
    "    'LA': '220',\n",
    "    'NC': '370',\n",
    "    'ND': '380',\n",
    "    'NE': '310',\n",
    "    'TN': '470',\n",
    "    'NY': '360',\n",
    "    'PA': '420',\n",
    "    'AK': '020',\n",
    "    'NV': '320',\n",
    "    'NH': '330',\n",
    "    'VA': '510',\n",
    "    'CO': '080',\n",
    "    'CA': '060',\n",
    "    'AL': '010',\n",
    "    'AR': '050',\n",
    "    'VT': '500',\n",
    "    'IL': '170',\n",
    "    'GA': '130',\n",
    "    'IN': '180',\n",
    "    'IA': '190',\n",
    "    'MA': '250',\n",
    "    'AZ': '040',\n",
    "    'ID': '160',\n",
    "    'CT': '090',\n",
    "    'ME': '230',\n",
    "    'MD': '240',\n",
    "    'OK': '400',\n",
    "    'OH': '390',\n",
    "    'UT': '490',\n",
    "    'MO': '290',\n",
    "    'MN': '270',\n",
    "    'MI': '260',\n",
    "    'RI': '440',\n",
    "    'KS': '200',\n",
    "    'MT': '300',\n",
    "    'MS': '280',\n",
    "    'SC': '450',\n",
    "    'KY': '210',\n",
    "    'OR': '410',\n",
    "    'SD': '460',\n",
    "    'PR': '720'\n",
    "}\n",
    "\n",
    "wd = \"D:/workspace/TIGER2017/shps/\"\n",
    "\n",
    "os.chdir(wd)\n",
    "\n",
    "zips = glob.glob(wd + \"US*.zip\")\n",
    "\n",
    "ziplist = []\n",
    "\n",
    "for name in zips:\n",
    "    file = os.path.basename(name)\n",
    "    ziplist.append(file)\n",
    "\n",
    "for f in ziplist:\n",
    "    new_zip = f.lower()\n",
    "    os.rename(f,new_zip)\n",
    "\n",
    "for state in states_upper:\n",
    "    in_zip = glob.glob(wd + state + \"*.zip\")\n",
    "    in_zip_string = in_zip[0]\n",
    "    in_zip_file = os.path.basename(in_zip_string)\n",
    "\n",
    "    out_zip_file = state_codes.get(state) + \"_blck_grp_2017.zip\"\n",
    "    os.rename(in_zip_file, out_zip_file)\n",
    "    \n",
    "                 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
